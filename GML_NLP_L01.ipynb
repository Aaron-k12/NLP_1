{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GML NLP L01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google ML - Natural Language Processing\n",
        "## Tokenization and Sequence generation"
      ],
      "metadata": {
        "id": "NndKIixstm5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenzation\n",
        "\n",
        "This generally involves breaking a sentence down into its constituent words.\n",
        "\n",
        "Tensorflow's tokenizer takes this a step further by representing the individual words by numerical values to make them easier for use in training a neural network to perform NLP."
      ],
      "metadata": {
        "id": "mlYJN5X3t3FK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmErXKhNyumF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    'i know basic programming',\n",
        "    'I know Matlab programming.',\n",
        "    'You know python programming!',\n",
        "    'Do you love the python programming simplicity?',\n",
        "    'I expertly know python programming',\n",
        "    'Programming in python beats programming in Malbolge.'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sentences[:3]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100) #tokenizes a max of 100 words\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "word_index = tokenizer.word_index #assigns index based on frequency\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyrKdJu2Z1LR",
        "outputId": "f4c9dfaa-01b0-46bc-c230-4ff01a5a12ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'know': 1, 'programming': 2, 'i': 3, 'basic': 4, 'matlab': 5, 'you': 6, 'python': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence Generation\n",
        "\n",
        "Creating sequences of numbers from sentences."
      ],
      "metadata": {
        "id": "WxOzIFDQxKux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sentences[:4]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "# tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Gir2PkZtNy",
        "outputId": "15c67b0d-c201-4462-bffc-99e62a0d2975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'programming': 1, 'know': 2, 'i': 3, 'you': 4, 'python': 5, 'basic': 6, 'matlab': 7, 'do': 8, 'love': 9, 'the': 10, 'simplicity': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After generating the tokens for the individual words in the senteces,\n",
        "# we represent the sentences as token sequences.\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(train_data)\n",
        "print(train_data)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z9FiJj3xi2J",
        "outputId": "63e6cb8e-24d0-4454-98dd-f484afafb870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i know basic programming', 'I know Matlab programming.', 'You know python programming!', 'Do you love the python programming simplicity?']\n",
            "[[3, 2, 6, 1], [3, 2, 7, 1], [4, 2, 5, 1], [8, 4, 9, 10, 5, 1, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs to ANNs should be of the same size\n",
        "# for variable input lengths,\n",
        "#   pad to largest input length\n",
        "#   resize to N input length\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "GZmixcLUyulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padded = pad_sequences(sequences, maxlen=#, padding='pre_or_post',\n",
        "#    truncating='pre_or_post')\n",
        "padded = pad_sequences(sequences)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIGMQIBG1R0H",
        "outputId": "fac26f6e-3b75-47bb-8df7-bf21308fdd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequences =  [[3, 2, 6, 1], [3, 2, 7, 1], [4, 2, 5, 1], [8, 4, 9, 10, 5, 1, 11]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 0  0  0  3  2  6  1]\n",
            " [ 0  0  0  3  2  7  1]\n",
            " [ 0  0  0  4  2  5  1]\n",
            " [ 8  4  9 10  5  1 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer with new data\n",
        "test_data = sentences[4:]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nWord Index = \",word_index)\n",
        "print(\"Test Sentences = \", test_data)\n",
        "print(\"Test Sequences = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "print(padded)\n",
        "\n",
        "## The resulted sequences are distored as it does have tokens\n",
        "## for words it has not fit. New words are skipped without consideration\n",
        "## Include the out-of-vocabulary (oov) token and fit tokenizer again."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3cWgbxK2l96",
        "outputId": "a4bde96c-bcc8-4172-f22e-28d7506fe09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'programming': 1, 'know': 2, 'i': 3, 'you': 4, 'python': 5, 'basic': 6, 'matlab': 7, 'do': 8, 'love': 9, 'the': 10, 'simplicity': 11}\n",
            "Test Sentences =  ['I expertly know python programming', 'Programming in python beats programming in Malbolge.']\n",
            "Test Sequences =  [[3, 2, 5, 1], [1, 5, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[3 2 5 1]\n",
            " [0 1 5 1]]\n"
          ]
        }
      ]
    }
  ]
}